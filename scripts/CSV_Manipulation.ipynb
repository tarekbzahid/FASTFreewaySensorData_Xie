{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read the CSV file into a Dask DataFrame\n",
    "df = dd.read_csv('data.csv')\n",
    "\n",
    "# Filter the DataFrame to only include rows where DetectorID = 656.1.503\n",
    "filtered_df = df[df['DetectorID'] == 656.1503]\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Read the detector IDs from the CSV file\n",
    "detector_ids = []\n",
    "with open('/home/tzahid/Desktop/Xie/data/detector_ids.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        detector_ids.append(row[0])\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"filter_detectorid\").getOrCreate()\n",
    "\n",
    "# Read the FASTFreewaySensorData2018.csv file into a Spark DataFrame\n",
    "df = spark.read.csv(\"/home/tzahid/Desktop/Xie/data/FASTFreewaySensorData2018.csv\", header=True)\n",
    "\n",
    "# Process each detector ID\n",
    "for detector_id in detector_ids:\n",
    "    # Convert the detector ID to the format with '.' instead of '_'\n",
    "    normalized_detector_id = detector_id.replace('_', '.')\n",
    "\n",
    "    # Filter the DataFrame to include rows where \"DetectorID\" is equal to the normalized detector ID\n",
    "    filtered_df = df.filter(df[\"DetectorID\"] == normalized_detector_id)\n",
    "\n",
    "    # Reduce the number of partitions to one using coalesce()\n",
    "    coalesced_df = filtered_df.coalesce(1)\n",
    "\n",
    "    # Construct the output file path using the detector ID\n",
    "    output_filepath = f\"/home/tzahid/Desktop/Xie/data/{detector_id}/{detector_id}.csv\"\n",
    "\n",
    "    # Save the coalesced DataFrame as a CSV file with headers\n",
    "    coalesced_df.write.option(\"header\", True).mode(\"overwrite\").csv(output_filepath)\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Creating a memory efficient \n",
    "\n",
    "def process_detector_id(detector_id):\n",
    "    # Normalize the detector ID\n",
    "    normalized_detector_id = detector_id.replace('_', '.')\n",
    "\n",
    "    # Create a SparkSession for this detector ID\n",
    "    spark = SparkSession.builder.appName(f\"filter_detectorid_{detector_id}\").getOrCreate()\n",
    "\n",
    "    # Read the FASTFreewaySensorData2018.csv file into a Spark DataFrame\n",
    "    df = spark.read.csv(\"/home/tzahid/Desktop/Xie/data/FASTFreewaySensorData2018.csv\", header=True)\n",
    "\n",
    "    # Filter the DataFrame to include rows where \"DetectorID\" is equal to the normalized detector ID\n",
    "    filtered_df = df.filter(df[\"DetectorID\"] == normalized_detector_id)\n",
    "\n",
    "    # Reduce the number of partitions to one using coalesce()\n",
    "    coalesced_df = filtered_df.coalesce(1)\n",
    "\n",
    "    # Construct the output file path using the detector ID\n",
    "    output_filepath = f\"/home/tzahid/Desktop/Xie/data/{detector_id}/\"\n",
    "\n",
    "    # Save the coalesced DataFrame as a CSV file with headers\n",
    "    coalesced_df.write.option(\"header\", True).mode(\"overwrite\").csv(output_filepath)\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the detector IDs from the CSV file\n",
    "    detector_ids = []\n",
    "    with open('/sensor_list - bugatti.nvfast.orgDetectors/I-15 NB.txt', 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            detector_ids.append(row[0])\n",
    "\n",
    "    # Process each detector ID sequentially\n",
    "    for detector_id in detector_ids:\n",
    "        process_detector_id(detector_id)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
